# -*- coding: utf-8 -*-
"""Reddit-stories-ai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1od4mxPwM6MoIE8KSkGQwT14f1VBfDSEK
"""
# Install Python packages
!pip install requests pandas praw
!pip install torch transformers tqdm
!pip install langdetect googletrans==4.0.0-rc1
!pip install pydub
!pip install git+https://github.com/openai/whisper.git
!pip install google-auth-oauthlib==0.4.1 google-auth==1.35.0 google-api-python-client==2.0.2
!pip install moviepy

# Install system packages
!apt-get -qq update
!apt-get -qq -y install espeak-ng ffmpeg

# Import necessary libraries
import requests
import pandas as pd
from datetime import datetime, timedelta
import time
import random
import praw  # Using PRAW library for more reliable Reddit API access

# Setup Reddit API using PRAW
def get_reddit_instance(client_id, client_secret, user_agent):
    """
    Create a Reddit instance using PRAW

    Parameters:
    - client_id: Your Reddit API client ID
    - client_secret: Your Reddit API client secret
    - user_agent: A unique identifier for your app

    Returns:
    - reddit: Authenticated Reddit instance
    """
    try:
        reddit = praw.Reddit(
            client_id=client_id,
            client_secret=client_secret,
            user_agent=user_agent,
            check_for_async=False
        )
        return reddit
    except Exception as e:
        print(f"Authentication failed: {e}")
        return None

# Get yesterday's posts from specified relationship subreddits
def get_yesterdays_relationship_posts(reddit, subreddit_names, max_posts=100):
    """
    Extract posts from multiple relationship subreddits that were posted yesterday,
    ordered by priority: upvotes first, shares second, comments last

    Parameters:
    - reddit: Authenticated Reddit instance
    - subreddit_names: List of subreddit names to fetch from
    - max_posts: Maximum total number of posts to return (across all subreddits)

    Returns:
    - DataFrame containing post details sorted by priority
    """
    try:
        # Calculate yesterday's date range
        today = datetime.now()
        yesterday = today - timedelta(days=1)
        yesterday_start = datetime(yesterday.year, yesterday.month, yesterday.day, 0, 0, 0)
        yesterday_end = datetime(yesterday.year, yesterday.month, yesterday.day, 23, 59, 59)

        # Convert to Unix timestamp
        yesterday_start_unix = int(yesterday_start.timestamp())
        yesterday_end_unix = int(yesterday_end.timestamp())

        # Create empty lists to store post details
        titles = []
        post_texts = []
        upvotes = []
        comments = []
        shares = []  # Adding shares tracking
        subreddit_sources = []

        # Process each subreddit
        print(f"Collecting posts from yesterday ({yesterday.strftime('%Y-%m-%d')})...")
        all_posts = []

        for subreddit_name in subreddit_names:
            print(f"Fetching posts from r/{subreddit_name}...")
            subreddit = reddit.subreddit(subreddit_name)

            # Get new posts (we need to fetch more to ensure we get enough from yesterday)
            # Fetch at least 200 to ensure we get enough posts from yesterday
            new_posts = subreddit.new(limit=300)

            # Extract posts from yesterday and store complete post objects
            for post in new_posts:
                # Check if post was created yesterday
                if yesterday_start_unix <= post.created_utc <= yesterday_end_unix:
                    all_posts.append(post)

                # Add a small delay to avoid rate limiting
                time.sleep(random.uniform(0.2, 0.5))

        print(f"Found {len(all_posts)} posts from yesterday across all subreddits.")

        # Sort posts by priority: upvotes first, shares second, comments last
        # Note: PRAW doesn't directly provide share counts, so we'll estimate using view count if available
        # or use a weighted combination where upvotes have highest weight
        all_posts.sort(key=lambda post: (
            post.score * 100,  # Upvotes (highest priority - multiplied by 100 for emphasis)
            getattr(post, 'view_count', 0) or 0,  # Shares (estimated from views, ensuring it's never None)
            post.num_comments  # Comments (lowest priority)
        ), reverse=True)

        # Take the top max_posts
        top_posts = all_posts[:max_posts]

        # Extract data from the top posts
        for post in top_posts:
            titles.append(post.title)
            post_texts.append(post.selftext)
            upvotes.append(post.score)
            comments.append(post.num_comments)
            # Try to get view count as a proxy for shares, otherwise use 0
            shares.append(getattr(post, 'view_count', 0))
            subreddit_sources.append(post.subreddit.display_name)

        # Create DataFrame with data
        posts_df = pd.DataFrame({
            'Title': titles,
            'Text': post_texts,
            'Upvotes': upvotes,
            'Shares': shares,  # Added shares column
            'Comments': comments,
            'Subreddit': subreddit_sources
        })

        return posts_df

    except Exception as e:
        print(f"Error getting posts: {e}")
        return None

# Main function to execute the script
def main():
    # Your Reddit API credentials
    client_id = "4hUOJ-UA4DNOMb9_YL0YYA"
    client_secret = "PDxE8AzkWPLGMqbPDQiQkHZ0gYSXsA"

    # Create a specific and unique user agent
    user_agent = "python:com.example.relationship_posts_collector:v1.0 (by /u/YOUR_REDDIT_USERNAME)"

    # List of subreddits to fetch from
    subreddits = ["relationships", "relationship_advice", "RelationshipIndia"]

    # Maximum number of posts to collect in total
    MAX_POSTS = 5

    print("Authenticating with Reddit API...")
    reddit = get_reddit_instance(client_id, client_secret, user_agent)

    if reddit is None:
        print("Authentication failed. Please check your credentials.")
        return

    print(f"Extracting yesterday's most popular posts from relationship subreddits (max: {MAX_POSTS} posts)...")
    posts_df = get_yesterdays_relationship_posts(reddit, subreddits, max_posts=MAX_POSTS)

    if posts_df is not None:
        # Display the results summary
        print(f"Successfully extracted {len(posts_df)} popular posts from yesterday.")

        # Show posts count by subreddit
        subreddit_counts = posts_df['Subreddit'].value_counts()
        for subreddit, count in subreddit_counts.items():
            print(f"- r/{subreddit}: {count} posts")

        # Show preview of top 5 posts
        print("\nPreview of top 5 most popular posts:")
        preview = posts_df[['Title', 'Upvotes', 'Shares', 'Comments', 'Subreddit']].head(5)
        print(preview)

        # Save to CSV with only title and text
        current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"popular_relationship_posts.csv"

        # Final output only includes title and text as requested
        final_df = posts_df[['Title', 'Text']]
        final_df.to_csv(filename, index=False)

        print(f"\nData saved to {filename}")
    else:
        print("Failed to extract posts.")

# Execute the script
if __name__ == "__main__":
    main()

import pandas as pd
import langdetect
from googletrans import Translator
import time
from tqdm import tqdm

# Initialize the translator
translator = Translator()

def detect_language(text):
    """Detect the language of the given text."""
    try:
        if pd.isna(text) or text.strip() == '':
            return 'unknown'
        return langdetect.detect(text)
    except:
        return 'unknown'

def translate_text(text, src_lang):
    """Translate text to English if it's not already in English."""
    if pd.isna(text) or text.strip() == '' or src_lang == 'en' or src_lang == 'unknown':
        return text

    try:
        # Add a small delay to avoid hitting API limits
        time.sleep(0.5)
        translation = translator.translate(text, src=src_lang, dest='en')
        return translation.text
    except Exception as e:
        print(f"Translation error: {e}")
        return text  # Return original text if translation fails

# Load the CSV file
file_path = 'popular_relationship_posts.csv'
df = pd.read_csv(file_path)

# Identify the text column(s)
potential_text_columns = ['Text']
text_columns = [col for col in df.columns if col in potential_text_columns]

if not text_columns:
    print(f"No text columns found. Available columns are: {df.columns.tolist()}")
    # If no predefined column names match, assume the first column with string data is the text column
    for col in df.columns:
        if df[col].dtype == 'object':
            text_columns = [col]
            print(f"Using column '{col}' for text processing")
            break

if not text_columns:
    print("No suitable text columns found!")
else:
    # Add a language detection column
    print("Processing text columns...")

    for col in text_columns:
        print(f"Processing column: {col}")

        # Store original text in a temporary column just for backup
        df['_temp_original'] = df[col].copy()

        # Detect language for each row and store in a language column
        print("Detecting languages...")
        df['text_lang'] = df[col].apply(detect_language)

        # Count languages detected
        lang_counts = df['text_lang'].value_counts()
        print("Languages detected:")
        print(lang_counts)

        # Translate non-English text and replace original text
        print("Translating non-English text...")
        non_english_rows = df['text_lang'] != 'en'
        total_to_translate = non_english_rows.sum()

        if total_to_translate > 0:
            print(f"Found {total_to_translate} rows with non-English text to translate")

            # Translate only non-English rows and update original column
            for idx in tqdm(df[non_english_rows].index):
                src_lang = df.at[idx, 'text_lang']
                original_text = df.at[idx, col]
                translated_text = translate_text(original_text, src_lang)
                df.at[idx, col] = translated_text

            print(f"Translated {total_to_translate} rows in column '{col}'")
        else:
            print("No non-English text found, no translation needed")

    # Remove the temporary column
    df.drop('_temp_original', axis=1, inplace=True)

    # Save the result with translated text
    output_file = 'translated_file.csv'
    df.to_csv(output_file, index=False)
    print(f"Saved results to {output_file}")

import pandas as pd
import numpy as np
from transformers import pipeline
import re
import nltk
from nltk.tokenize import word_tokenize
import warnings
warnings.filterwarnings('ignore')

# Download necessary NLTK data
import os

# Create a directory for NLTK data in the current working directory
nltk_data_dir = os.path.join(os.getcwd(), 'nltk_data')
os.makedirs(nltk_data_dir, exist_ok=True)

# Download the required NLTK resources to the local directory
nltk.download('punkt', download_dir=nltk_data_dir, quiet=True)
nltk.data.path.append(nltk_data_dir)  # Add our directory to NLTK's search path

def count_words(text):
    """Count the number of words in a text."""
    if pd.isna(text) or text == "":
        return 0

    text = str(text)

    # Use a simple but robust tokenization approach
    text = re.sub(r'([.,!?;:])', r' \1 ', text)
    text = re.sub(r'\s+', ' ', text)
    words = [w for w in text.split() if w.strip()]
    return len(words)

def clean_text(text):
    """Clean the text by removing URLs, extra spaces, etc."""
    if pd.isna(text) or text == "":
        return ""

    # Convert to string if it's not already
    text = str(text)

    # Remove URLs
    text = re.sub(r'http\S+', '', text)

    # Remove multiple spaces
    text = re.sub(r'\s+', ' ', text)

    return text.strip()

def adjust_summary_length(text, target_min=250, target_max=300):
    """Adjust the summary to be between target_min and target_max words."""
    word_count = count_words(text)

    # If already in the desired range, return as is
    if target_min <= word_count <= target_max:
        return text

    # If too long, truncate
    if word_count > target_max:
        words = text.split()
        return ' '.join(words[:target_max])

    # For summaries that are too short, we'll keep the original
    # (this shouldn't happen with our parameters but added for robustness)
    return text

def main():
    # Specify input file path as requested
    input_file = '/content/translated_file.csv'
    output_file = 'final_text.csv'

    print(f"Loading CSV data from {input_file}...")
    try:
        # Load the CSV file
        df = pd.read_csv(input_file)
        print(f"Successfully loaded CSV with {len(df)} rows")

        # Print the column names to help with debugging
        print(f"Columns in the CSV: {df.columns.tolist()}")

        # Determine which column contains the post content
        content_columns = [col for col in df.columns if any(term in col.lower()
                          for term in ['text', 'content', 'post', 'body', 'selftext'])]

        if not content_columns:
            print("Could not find a column that likely contains post content.")
            print("Please provide the name of the column containing post text:")
            content_column = input()
        else:
            content_column = content_columns[0]
            print(f"Using column '{content_column}' for post content")

        # Clean the text data
        print("Cleaning text data...")
        df['cleaned_text'] = df[content_column].apply(clean_text)

        # Count words in each post
        print("Counting words in each post...")
        df['word_count'] = df['cleaned_text'].apply(count_words)

        # Initialize summarization only if needed
        long_posts = len(df[df['word_count'] > 300])
        if long_posts > 0:
            print(f"Found {long_posts} posts longer than 300 words that need summarization")

            try:
                # Initialize the summarization model
                print("Loading summarization model...")
                summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

                # Process each row with minimal messaging
                summarized_texts = []
                total_posts = len(df)

                for idx, row in df.iterrows():
                    if idx % 20 == 0:  # Show progress less frequently
                        print(f"Processing item {idx+1}/{total_posts} ({(idx+1)/total_posts*100:.1f}%)")

                    # If text is already under 300 words, keep it as is
                    if row['word_count'] <= 300:
                        summarized_texts.append(row['cleaned_text'])
                        continue

                    # For longer texts, we need to summarize to between 250-300 words
                    try:
                        text = row['cleaned_text']

                        # Set summarization parameters to target our desired range
                        # We aim for 250-270 words from the model, then will adjust if needed
                        target_length = 260
                        max_length = target_length
                        min_length = 250

                        # Attempt summarization with error handling
                        try:
                            summary = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)
                            summary_text = summary[0]['summary_text']

                            # Adjust the summary to be within our target range
                            summary_text = adjust_summary_length(summary_text, 250, 300)
                        except:
                            # Fall back to truncation if summarization fails
                            words = text.split()
                            summary_text = ' '.join(words[:275])  # Aim for middle of range

                        summarized_texts.append(summary_text)
                    except:
                        # Silently fail and use truncation as a last resort
                        words = row['cleaned_text'].split()
                        summarized_texts.append(' '.join(words[:275]))

                # Assign the results back to the dataframe
                df['summarized_text'] = summarized_texts

                # Double-check word counts to ensure we're in the right range
                df['final_word_count'] = df['summarized_text'].apply(count_words)

                # Report the results
                in_range = df[(df['word_count'] > 300) & (250 <= df['final_word_count']) & (df['final_word_count'] <= 300)]
                print(f"Successfully summarized {len(in_range)} of {long_posts} long posts to 250-300 words")

                # Make any final adjustments if needed
                df['summarized_text'] = df.apply(
                    lambda row: row['summarized_text'] if row['word_count'] <= 300 or (250 <= row['final_word_count'] <= 300)
                    else adjust_summary_length(row['summarized_text'], 250, 300),
                    axis=1
                )

                print("Summarization complete!")

            except Exception as e:
                print(f"Summarization error: {e}")
                print("Falling back to simple truncation for all posts")
                # Fallback: truncate long posts to 275 words (middle of our target range)
                df['summarized_text'] = df.apply(
                    lambda row: row['cleaned_text'] if row['word_count'] <= 300
                    else ' '.join(row['cleaned_text'].split()[:275]),
                    axis=1
                )
        else:
            print("No posts require summarization, using original text")
            df['summarized_text'] = df['cleaned_text']

        # Make sure we only keep the final summarized_text column
        # (remove any duplicate attempts from the code)
        if 'summarized_text' in df.columns:
            # Keep only essential columns in final output
            result_df = df[[content_column, 'summarized_text']]

            # Save results to specified output file
            print(f"Saving results to {output_file}...")
            result_df.to_csv(output_file, index=False)

            print(f"Process complete! Saved {len(df)} processed posts.")

            # Print summary statistics
            if 'final_word_count' in df.columns:
                long_summarized = df[df['word_count'] > 300]
                avg_length = long_summarized['final_word_count'].mean()
                print(f"Average summary length: {avg_length:.1f} words")
                print(f"Summaries in target range (250-300): {len(long_summarized[(250 <= long_summarized['final_word_count']) & (long_summarized['final_word_count'] <= 300)])} of {len(long_summarized)}")
        else:
            print("Error: Failed to generate summarized text column")

    except Exception as e:
        print(f"An error occurred: {e}")

if __name__ == "__main__":
    main()

import pandas as pd
import torch
import soundfile as sf
from IPython.display import display, Audio
import os
import numpy as np
from pydub import AudioSegment
import re

# Voice configuration - only two voices as requested
VOICES = {
    "female": "af_heart",  # Female voice
    "male": "am_adam"      # Male voice
}

# Create output directory if it doesn't exist
os.makedirs("story_audio", exist_ok=True)
os.makedirs("story_audio/temp_segments", exist_ok=True)

# Function to install required packages if they're missing
def install_required_packages():
    try:
        import pip
        required_packages = ["pydub", "numpy", "soundfile", "pandas"]
        for package in required_packages:
            try:
                __import__(package)
            except ImportError:
                print(f"Installing required package: {package}")
                pip.main(["install", package])

        # Try to install kokoro automatically
        try:
            pip.main(["install", "kokoro"])
            print("Kokoro installed successfully")
        except:
            print("Kokoro installation failed")

    except Exception as e:
        print(f"Failed to install required packages: {e}")
        print("Please manually install: pydub, numpy, soundfile, pandas")

# Install required packages
install_required_packages()

# Automatically load the CSV file - use default if not found
try:
    csv_file = 'final_text.csv'
    df = pd.read_csv(csv_file)
    print(f"Successfully loaded CSV with {len(df)} entries")
except FileNotFoundError:
    print(f"CSV file {csv_file} not found. Creating sample data for demonstration.")
    # Create sample data for demonstration
    df = pd.DataFrame({
        'summarized_text_translated': [
            "Once upon a time in a small village, there lived a young girl who discovered she could talk to animals.",
            "The spaceship landed quietly in the desert. Its occupants were not what the scientists expected.",
            "The old bookstore held a secret passage that only opened during a full moon.",
            "Every night at midnight, the clocktower would play a melody that no one remembered placing there.",
            "The detective knew the answer was hidden somewhere in the painting."
        ]
    })
    print("Created sample data for demonstration")

# Identify the column containing the story text
text_column = None
possible_columns = ['summarized_text']

for col in possible_columns:
    if col in df.columns:
        text_column = col
        break

if text_column is None:
    print(f"Could not find text column. Available columns: {df.columns.tolist()}")
    text_column = df.columns[0]  # Use the first column as fallback
    print(f"Using '{text_column}' as fallback")

print(f"Using column '{text_column}' for text data")

# Function to determine if a story has female or male narration
def determine_gender(text):
    """
    Simple heuristic to determine if a story is likely narrated by a female or male character.
    This is a basic implementation and could be improved with NLP techniques.
    """
    # Look for common female pronouns/indicators
    female_indicators = ['she', 'her', 'herself', 'woman', 'girl', 'mother', 'daughter', 'sister', 'aunt']
    # Look for common male pronouns/indicators
    male_indicators = ['he', 'him', 'himself', 'man', 'boy', 'father', 'son', 'brother', 'uncle']

    # Convert to lowercase for case-insensitive matching
    text_lower = text.lower()

    # Count occurrences
    female_count = sum([text_lower.count(' ' + word + ' ') for word in female_indicators])
    male_count = sum([text_lower.count(' ' + word + ' ') for word in male_indicators])

    # Return gender based on counts
    if female_count > male_count:
        return "female"
    else:
        return "male"  # Default to male if equal or more male indicators

# Process all entries in the dataset
num_entries = len(df)
print(f"Processing all {num_entries} entries from the dataset")

# Determine voice for each story based on content
story_voices = {}
for i in range(num_entries):
    text = df[text_column].iloc[i]
    gender = determine_gender(text)
    story_voices[i] = VOICES[gender]
    print(f"Story {i+1}: Detected as {gender} narration, using voice {VOICES[gender]}")

# Function for TTS conversion with segment joining
def text_to_speech(text, voice, output_path, sample_rate=24000):
    """
    Convert text to speech using the specified voice and join all segments.

    Args:
        text: The text to convert to speech
        voice: The voice to use
        output_path: Where to save the final joined audio
        sample_rate: Sample rate of the audio (default: 24000)
    """
    print(f"Converting text using voice: {voice}")
    print(f"Text: {text[:100]}..." if len(text) > 100 else f"Text: {text}")

    # Create a temp directory for segments
    temp_dir = os.path.join(os.path.dirname(output_path), "temp_segments")
    os.makedirs(temp_dir, exist_ok=True)

    try:
        # Initialize Kokoro pipeline if using it
        from kokoro import KPipeline
        pipeline = KPipeline(lang_code='a')

        # Generate audio segments
        all_audio_data = []
        segment_paths = []

        # Use the generator to create segments
        generator = pipeline(text, voice=voice)

        for i, (gs, ps, audio) in enumerate(generator):
            # Save each segment temporarily
            segment_path = os.path.join(temp_dir, f"{os.path.basename(output_path).split('.')[0]}_seg_{i}.wav")
            segment_paths.append(segment_path)
            sf.write(segment_path, audio, sample_rate)
            all_audio_data.append(audio)
            print(f"Generated segment {i+1}")

        # Join all audio segments into one file
        join_audio_segments(segment_paths, output_path, sample_rate)
        print(f"All segments joined and saved to: {output_path}")

        # Clean up temporary segment files
        for segment_path in segment_paths:
            if os.path.exists(segment_path):
                os.remove(segment_path)

        return True

    except ImportError:
        print("Kokoro library not found. Using simulation mode.")
        # For demonstration purposes (when library is not available)
        with open(output_path.replace('.wav', '.txt'), 'w') as f:
            f.write(f"Voice: {voice}\n\n{text}")
        return True
    except Exception as e:
        print(f"Error in text-to-speech conversion: {e}")
        return False


def join_audio_segments(segment_paths, output_path, sample_rate=24000):
    """
    Join multiple audio segments into a single file.

    Args:
        segment_paths: List of paths to audio segment files
        output_path: Path to save the joined audio
        sample_rate: Audio sample rate
    """
    # Method 1: Using pydub (more robust, handles different formats)
    try:
        # Check if we have any segments
        if not segment_paths:
            print("No segments to join")
            return False

        # Join with pydub
        combined = AudioSegment.empty()
        for path in segment_paths:
            sound = AudioSegment.from_file(path)
            combined += sound

        # Export the combined audio
        combined.export(output_path, format="wav")
        return True

    except Exception as e:
        print(f"Error joining with pydub: {e}")
        # Fallback to numpy method
        try:
            # Method 2: Using numpy and soundfile (simpler, but less flexible)
            all_audio = []
            for path in segment_paths:
                audio_data, _ = sf.read(path)
                all_audio.append(audio_data)

            # Concatenate all audio data
            combined_audio = np.concatenate(all_audio)

            # Write combined audio to file
            sf.write(output_path, combined_audio, sample_rate)
            return True

        except Exception as e2:
            print(f"Error joining with numpy: {e2}")
            return False

# Process the entries
for i in range(num_entries):
    text = df[text_column].iloc[i]
    voice = story_voices[i]

    if not isinstance(text, str) or len(text.strip()) == 0:
        print(f"Skipping empty or non-string entry at index {i}")
        continue

    print(f"\n--- Processing Story {i+1}/{num_entries} ---")
    output_file = f"story_audio/story_{i+1}_{voice}.wav"

    # Convert long text into manageable chunks if needed
    if len(text) > 1000:
        # Split text into sentences first
        sentences = re.split('(?<=[.!?])\s+', text)

        # Group sentences into chunks of reasonable size
        chunks = []
        current_chunk = ""

        for sentence in sentences:
            if len(current_chunk) + len(sentence) < 1000:
                current_chunk += " " + sentence if current_chunk else sentence
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = sentence

        # Add the last chunk if it exists
        if current_chunk:
            chunks.append(current_chunk)

        print(f"Text split into {len(chunks)} chunks for processing")

        # Process each chunk separately and collect segment paths
        segment_paths = []
        for j, chunk in enumerate(chunks):
            chunk_path = f"story_audio/temp_chunk_{i+1}_{j+1}.wav"
            success = text_to_speech(chunk, voice, chunk_path)
            if success:
                segment_paths.append(chunk_path)
                print(f"Successfully processed chunk {j+1}/{len(chunks)} for Story {i+1}")
            else:
                print(f"Failed to process chunk {j+1}/{len(chunks)} for Story {i+1}")

        # Join all chunks into one final story audio
        if segment_paths:
            join_audio_segments(segment_paths, output_file)
            print(f"Successfully joined all chunks for Story {i+1} with voice {voice}")

            # Clean up temporary chunk files
            for path in segment_paths:
                if os.path.exists(path):
                    os.remove(path)
    else:
        # Process text directly if it's not too long
        success = text_to_speech(text, voice, output_file)
        if success:
            print(f"Successfully processed Story {i+1} with voice {voice}")
        else:
            print(f"Failed to process Story {i+1}")

# Print voice assignments for reference
print("\nVoice assignments summary:")
gender_counts = {"female": 0, "male": 0}
for i in range(num_entries):
    voice = story_voices[i]
    if voice == VOICES["female"]:
        gender_counts["female"] += 1
    else:
        gender_counts["male"] += 1

print(f"Female voice stories: {gender_counts['female']}")
print(f"Male voice stories: {gender_counts['male']}")

print("\nProcessing complete!")

# Automatically combine all stories into one audio file
all_story_files = [f"story_audio/story_{i+1}_{story_voices[i]}.wav" for i in range(num_entries)]
all_story_files = [f for f in all_story_files if os.path.exists(f)]

# Define all_stories_path
all_stories_path = "story_audio/all_stories.wav"  # Or any desired path


if all_story_files:
    print(f"\nCombining all {len(all_story_files)} stories into one audio file...")
    join_audio_segments(all_story_files, all_stories_path)
    print(f"All stories combined into: {all_stories_path}")
else:
    print("No story audio files found to combine.")

#!/usr/bin/env python3
import os
import subprocess
import sys

def check_ytdlp_installation():
    """Check if yt-dlp is installed, and install it if not."""
    try:
        subprocess.run(["yt-dlp", "--version"], capture_output=True, text=True)
        print("yt-dlp is already installed.")
    except FileNotFoundError:
        print("yt-dlp not found. Installing...")
        try:
            if os.name == 'nt':  # Windows
                subprocess.run([sys.executable, "-m", "pip", "install", "yt-dlp"], check=True)
            else:  # Linux/Mac
                subprocess.run([sys.executable, "-m", "pip", "install", "yt-dlp"], check=True)
            print("yt-dlp installed successfully.")
        except subprocess.CalledProcessError:
            print("Failed to install yt-dlp. Please install it manually.")
            sys.exit(1)

def create_videos_folder():
    """Create a 'videos' folder if it doesn't exist."""
    if not os.path.exists("videos"):
        os.makedirs("videos")
        print("Created 'videos' folder.")
    else:
        print("'videos' folder already exists.")

def download_videos(video_urls):
    """Download videos at the best resolution and save to the videos folder."""
    for i, url in enumerate(video_urls, 1):
        print(f"\nDownloading video {i}/{len(video_urls)}: {url}")

        # Format: videos/%(title)s.%(ext)s
        # -f best: select best quality format
        # --no-playlist: Download only the video, not the playlist it might be in
        cmd = [
            "yt-dlp",
            "-f", "bestvideo+bestaudio/best",  # Best video+audio or best available if combined
            "-o", "videos/%(title)s.%(ext)s",
            "--no-playlist",
            url
        ]

        try:
            subprocess.run(cmd, check=True)
            print(f"Successfully downloaded video {i}")
        except subprocess.CalledProcessError as e:
            print(f"Error downloading video {i}: {e}")

def main():
    # List of YouTube video URLs to download
    video_urls = [
        "https://youtu.be/P3b3AidldmA?si=6BwoN-aV35FIxUiQ",
        "https://youtu.be/cjxxE2gwEVg?si=MjUwnuDCClngt5BI",
        "https://youtu.be/LZzwQrXIDEM?si=7y2BpxZfNrmkdo4A"
    ]

    print("YouTube Video Downloader")
    print("-----------------------")

    # Check for yt-dlp installation
    check_ytdlp_installation()

    # Create videos folder
    create_videos_folder()

    # Download videos
    download_videos(video_urls)

    print("\nAll downloads completed!")

if __name__ == "__main__":
    main()

!pip install moviepy

import os
import glob
from moviepy.editor import VideoFileClip, AudioFileClip, concatenate_videoclips

def sync_videos_with_audios(audio_dir='/content/story_audio', video_dir='/content/videos', output_dir='./shorts'):
    """
    Sync audio files with video content sequentially:
    - Each audio is added to the current video position
    - When a video is filled or nearly filled (less than audio duration remaining), move to next video
    - If all videos are used, cycle back to the first video

    Args:
        audio_dir: Directory containing audio files
        video_dir: Directory containing video files
        output_dir: Directory where output files will be saved
    """
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)

    # Debug: Print absolute paths to help diagnose issues
    print(f"Audio directory (absolute): {os.path.abspath(audio_dir)}")
    print(f"Video directory (absolute): {os.path.abspath(video_dir)}")

    # Debug: List all files in the audio directory
    print("Files in audio directory:")
    all_files_in_audio = os.listdir(audio_dir)
    for file in all_files_in_audio:
        print(f"  - {file}")

    # Get all audio files with more explicit extension matching
    audio_extensions = ('.mp3', '.wav', '.ogg', '.m4a', '.aac')
    audio_files = []

    for ext in audio_extensions:
        pattern = os.path.join(audio_dir, f'*{ext}')
        audio_files.extend(glob.glob(pattern))
        # Also try lowercase extension
        pattern = os.path.join(audio_dir, f'*{ext.lower()}')
        audio_files.extend(glob.glob(pattern))

    # Remove duplicates and sort
    audio_files = sorted(list(set(audio_files)))

    if not audio_files:
        print(f"No audio files found in {audio_dir}")
        print("Checking if files exist directly:")
        example_file = os.path.join(audio_dir, 'story_10_af_heart.wav')
        if os.path.exists(example_file):
            print(f"  - File exists: {example_file}")
            print("  - But glob pattern didn't match it. Trying direct file listing...")

            # Direct file listing as a fallback
            audio_files = [os.path.join(audio_dir, f) for f in all_files_in_audio
                          if f.lower().endswith(audio_extensions)]
            print(f"Found {len(audio_files)} audio files via direct listing")
        else:
            print(f"  - File doesn't exist: {example_file}")

        if not audio_files:
            return

    # Get all video files with more explicit extension matching
    video_extensions = ('.mp4', '.webm', '.mov', '.mkv', '.avi')
    video_files = []

    for ext in video_extensions:
        pattern = os.path.join(video_dir, f'*{ext}')
        video_files.extend(glob.glob(pattern))
        # Also try lowercase extension
        pattern = os.path.join(video_dir, f'*{ext.lower()}')
        video_files.extend(glob.glob(pattern))

    # Remove duplicates and sort
    video_files = sorted(list(set(video_files)))

    if not video_files:
        print(f"No video files found in {video_dir}")
        print("Listing all files in video directory:")
        all_files_in_video = os.listdir(video_dir)
        for file in all_files_in_video:
            print(f"  - {file}")
        return

    print(f"Found {len(audio_files)} audio files and {len(video_files)} video files")
    print("Audio files:")
    for af in audio_files[:5]:  # Print first 5 for verification
        print(f"  - {os.path.basename(af)}")
    if len(audio_files) > 5:
        print(f"  - ... and {len(audio_files) - 5} more")

    print("Video files:")
    for vf in video_files[:5]:  # Print first 5 for verification
        print(f"  - {os.path.basename(vf)}")
    if len(video_files) > 5:
        print(f"  - ... and {len(video_files) - 5} more")

    # Initialize variables to track current position in videos
    current_video_index = 0
    current_position_in_video = 0

    # Process each audio file
    for i, audio_path in enumerate(audio_files):
        audio_name = os.path.basename(audio_path)
        print(f"\nProcessing audio {i+1}/{len(audio_files)}: {audio_name}")

        try:
            # Load the audio and get its duration
            audio = AudioFileClip(audio_path)
            audio_duration = audio.duration
            print(f"Audio duration: {audio_duration:.2f}s")

            # Load the current video
            video_path = video_files[current_video_index]
            video_name = os.path.basename(video_path)
            print(f"Using video: {video_name} (starting at {current_position_in_video:.2f}s)")

            video = VideoFileClip(video_path)
            remaining_video_duration = video.duration - current_position_in_video

            # Check if we need to move to the next video
            # Move to next video if current position + audio duration > video duration
            # or if less than 1-2 seconds remain in the video
            if remaining_video_duration < audio_duration:
                print(f"Not enough space in current video (only {remaining_video_duration:.2f}s left). Moving to next video.")

                # Move to the next video
                current_video_index = (current_video_index + 1) % len(video_files)
                current_position_in_video = 0

                # Close the current video to free resources
                video.close()

                # Load the new video
                video_path = video_files[current_video_index]
                video_name = os.path.basename(video_path)
                print(f"Switched to video: {video_name} (starting at beginning)")
                video = VideoFileClip(video_path)

            # Create a subclip from the current position
            video_clip = video.subclip(current_position_in_video, current_position_in_video + audio_duration)

            # Replace the audio
            video_clip = video_clip.set_audio(audio)

            # Generate output filename with story_N format
            output_name = f"story_{i+1}.mp4"
            output_path = os.path.join(output_dir, output_name)

            # Write the final video
            print(f"Writing output to: {output_path}")
            video_clip.write_videofile(
                output_path,
                codec='libx264',
                audio_codec='aac',
                temp_audiofile='temp-audio.m4a',
                remove_temp=True,
                threads=4
            )

            # Update the current position in the video
            current_position_in_video += audio_duration

            # Check if we've reached the end of the current video
            if video.duration - current_position_in_video < 2:  # Less than 2 seconds remaining
                print(f"Less than 2 seconds remaining in current video. Will use next video for next audio.")
                current_video_index = (current_video_index + 1) % len(video_files)
                current_position_in_video = 0

            # Close clips to free up resources
            video_clip.close()
            audio.close()
            video.close()

        except Exception as e:
            print(f"Error processing {audio_name}: {str(e)}")
            # If there's an error, move to the next audio file but keep track of position
            continue

    print("\nAll processing complete!")

def create_single_combined_video(audio_dir='/content/story_audio', video_dir='/content/videos', output_dir='./shorts', output_name='combined_stories.mp4'):
    """
    Creates a single video with all audios sequentially added to videos:
    - Each audio is added to the current video position
    - When a video is filled, move to next video
    - If all videos are used, cycle back to the first video
    - All individual segments are concatenated into one final video

    Args:
        audio_dir: Directory containing audio files
        video_dir: Directory containing video files
        output_dir: Directory where output file will be saved
        output_name: Name of the final combined video file
    """
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)

    # Debug: Print absolute paths to help diagnose issues
    print(f"Audio directory (absolute): {os.path.abspath(audio_dir)}")
    print(f"Video directory (absolute): {os.path.abspath(video_dir)}")

    # Debug: List all files in the audio directory
    print("Files in audio directory:")
    all_files_in_audio = os.listdir(audio_dir)
    for file in all_files_in_audio:
        print(f"  - {file}")

    # Get all audio files with more explicit extension matching
    audio_extensions = ('.mp3', '.wav', '.ogg', '.m4a', '.aac')
    audio_files = []

    for ext in audio_extensions:
        pattern = os.path.join(audio_dir, f'*{ext}')
        audio_files.extend(glob.glob(pattern))
        # Also try lowercase extension
        pattern = os.path.join(audio_dir, f'*{ext.lower()}')
        audio_files.extend(glob.glob(pattern))

    # Remove duplicates and sort
    audio_files = sorted(list(set(audio_files)))

    if not audio_files:
        print(f"No audio files found in {audio_dir}")
        print("Checking if files exist directly:")
        example_file = os.path.join(audio_dir, 'story_10_af_heart.wav')
        if os.path.exists(example_file):
            print(f"  - File exists: {example_file}")
            print("  - But glob pattern didn't match it. Trying direct file listing...")

            # Direct file listing as a fallback
            audio_files = [os.path.join(audio_dir, f) for f in all_files_in_audio
                          if f.lower().endswith(audio_extensions)]
            print(f"Found {len(audio_files)} audio files via direct listing")
        else:
            print(f"  - File doesn't exist: {example_file}")

        if not audio_files:
            return

    # Get all video files with more explicit extension matching
    video_extensions = ('.mp4', '.webm', '.mov', '.mkv', '.avi')
    video_files = []

    for ext in video_extensions:
        pattern = os.path.join(video_dir, f'*{ext}')
        video_files.extend(glob.glob(pattern))
        # Also try lowercase extension
        pattern = os.path.join(video_dir, f'*{ext.lower()}')
        video_files.extend(glob.glob(pattern))

    # Remove duplicates and sort
    video_files = sorted(list(set(video_files)))

    if not video_files:
        print(f"No video files found in {video_dir}")
        print("Listing all files in video directory:")
        all_files_in_video = os.listdir(video_dir)
        for file in all_files_in_video:
            print(f"  - {file}")
        return

    print(f"Found {len(audio_files)} audio files and {len(video_files)} video files")
    print("Audio files:")
    for af in audio_files[:5]:  # Print first 5 for verification
        print(f"  - {os.path.basename(af)}")
    if len(audio_files) > 5:
        print(f"  - ... and {len(audio_files) - 5} more")

    print("Video files:")
    for vf in video_files[:5]:  # Print first 5 for verification
        print(f"  - {os.path.basename(vf)}")
    if len(video_files) > 5:
        print(f"  - ... and {len(video_files) - 5} more")

    # Initialize variables to track current position in videos
    current_video_index = 0
    current_position_in_video = 0
    video_segments = []

    # Process each audio file
    for i, audio_path in enumerate(audio_files):
        audio_name = os.path.basename(audio_path)
        print(f"\nProcessing audio {i+1}/{len(audio_files)}: {audio_name}")

        try:
            # Load the audio and get its duration
            audio = AudioFileClip(audio_path)
            audio_duration = audio.duration
            print(f"Audio duration: {audio_duration:.2f}s")

            # Load the current video
            video_path = video_files[current_video_index]
            video_name = os.path.basename(video_path)
            print(f"Using video: {video_name} (starting at {current_position_in_video:.2f}s)")

            video = VideoFileClip(video_path)
            remaining_video_duration = video.duration - current_position_in_video

            # Check if we need to move to the next video
            if remaining_video_duration < audio_duration:
                print(f"Not enough space in current video (only {remaining_video_duration:.2f}s left). Moving to next video.")

                # Move to the next video
                current_video_index = (current_video_index + 1) % len(video_files)
                current_position_in_video = 0

                # Close the current video to free resources
                video.close()

                # Load the new video
                video_path = video_files[current_video_index]
                video_name = os.path.basename(video_path)
                print(f"Switched to video: {video_name} (starting at beginning)")
                video = VideoFileClip(video_path)

            # Create a subclip from the current position
            video_clip = video.subclip(current_position_in_video, current_position_in_video + audio_duration)

            # Replace the audio
            video_clip = video_clip.set_audio(audio)

            # Add to segments list
            video_segments.append(video_clip)

            # Update the current position in the video
            current_position_in_video += audio_duration

            # Check if we've reached the end of the current video
            if video.duration - current_position_in_video < 2:  # Less than 2 seconds remaining
                print(f"Less than 2 seconds remaining in current video. Will use next video for next audio.")
                current_video_index = (current_video_index + 1) % len(video_files)
                current_position_in_video = 0

            # Close resources except for the video_clip which we'll keep for concatenation
            audio.close()
            video.close()

        except Exception as e:
            print(f"Error processing {audio_name}: {str(e)}")
            continue

    if video_segments:
        # Concatenate all segments into one video
        print("\nConcatenating all segments into one video...")
        final_video = concatenate_videoclips(video_segments)

        # Write the final video
        output_path = os.path.join(output_dir, output_name)
        print(f"Writing combined output to: {output_path}")
        final_video.write_videofile(
            output_path,
            codec='libx264',
            audio_codec='aac',
            temp_audiofile='temp-audio.m4a',
            remove_temp=True,
            threads=4
        )

        # Close clips
        final_video.close()
        for clip in video_segments:
            clip.close()

    print("\nAll processing complete!")

# Example usage
if __name__ == "__main__":
    # Update these paths to match your environment
    audio_dir = '/content/story_audio'  # Folder with audio files
    video_dir = '/content/videos'       # Folder with video files
    output_dir = 'shorts'             # Output folder

    # Choose which implementation to use
    # Option 1: Create individual videos for each audio
    sync_videos_with_audios(audio_dir, video_dir, output_dir)

    # Option 2: Create a single combined video with all audios
    # create_single_combined_video(audio_dir, video_dir, output_dir)

import os
import subprocess
import tempfile
import shutil
import whisper
import json
import csv
import re
from datetime import timedelta
from google.colab import files
from IPython.display import HTML, display

def add_subtitles_and_generate_metadata(input_folder="shorts", output_folder="subtitled", font="Roboto"):
    """
    Add word-by-word highlighted subtitles to videos in the input folder and generate metadata CSV.
    Subtitles appear line by line in the middle of the screen, with the current word highlighted in yellow.
    Uses the specified font (default: Roboto).
    Also generates video metadata including titles, descriptions, captions, tags and age restrictions.
    """
    # Create output folder if it doesn't exist
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    # Check if input folder exists
    if not os.path.exists(input_folder):
        print(f"Input folder '{input_folder}' not found. Creating it...")
        os.makedirs(input_folder)
        print(f"Please upload your videos to the '{input_folder}' folder and run this function again.")
        return

    # Find all video files in the input folder
    video_files = [f for f in os.listdir(input_folder) if f.lower().endswith(('.mp4', '.mov', '.avi'))]

    if not video_files:
        print(f"No video files found in '{input_folder}' folder. Please upload videos first.")
        return

    print(f"Found {len(video_files)} videos to process.")

    # Create a temporary work directory
    work_dir = os.path.join(tempfile.gettempdir(), "subtitle_work")
    os.makedirs(work_dir, exist_ok=True)

    # Download Roboto font if needed
    font_path = ensure_roboto_font(work_dir)

    # Load the Whisper model once
    print("Loading Whisper model (this may take a moment)...")
    model = whisper.load_model("base")
    print("Model loaded successfully!")

    # Create CSV file for metadata
    csv_path = "video_metadata.csv"
    with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:
        csv_writer = csv.writer(csvfile)
        # Write header row
        csv_writer.writerow([
            'Video File',
            'Output File',
            'Title',
            'Description',
            'Captions',
            'Tags',
            'Age Restriction'
        ])

        # Process each video
        for video_file in video_files:
            input_path = os.path.join(input_folder, video_file)
            base_name = os.path.splitext(video_file)[0]
            output_path = os.path.join(output_folder, f"{base_name}_subtitled.mp4")

            print(f"\nProcessing: {video_file}")

            try:
                # 1. Extract audio from video
                print("  Extracting audio...")
                audio_path = os.path.join(work_dir, f"{base_name}.wav")
                extract_audio(input_path, audio_path)

                # 2. Generate word-level transcription
                print("  Transcribing audio with word timestamps...")
                result = transcribe_with_word_timestamps(model, audio_path)

                # Save transcript for reference
                transcript_json = os.path.join(work_dir, f"{base_name}.json")
                with open(transcript_json, 'w', encoding='utf-8') as f:
                    json.dump(result, f, indent=2)

                # Get full transcript text for metadata generation
                full_transcript = result["text"]

                # 3. Create ASS subtitle file with word highlighting
                print("  Creating subtitle file with word highlighting...")
                ass_path = os.path.join(work_dir, f"{base_name}.ass")
                create_word_highlighting_ass(result, ass_path, font_path)

                # 4. Burn subtitles into video
                print("  Burning subtitles into video...")
                burn_subtitles(input_path, ass_path, output_path)

                # 5. Generate video metadata
                print("  Generating video metadata...")
                metadata = generate_video_metadata(base_name, full_transcript)

                # Write metadata to CSV
                csv_writer.writerow([
                    video_file,
                    f"{base_name}_subtitled.mp4",
                    metadata['title'],
                    metadata['description'],
                    metadata['captions'],
                    '; '.join(metadata['tags']),
                    metadata['age_restriction']
                ])

                print(f"✅ Successfully created: {output_path}")

            except Exception as e:
                print(f"❌ Failed to process {video_file}: {str(e)}")
                if os.path.exists(output_path):
                    os.remove(output_path)

    # Clean up temporary files
    shutil.rmtree(work_dir)

    print(f"\nAll videos processed! Results saved to '{output_folder}' folder.")
    print(f"Video metadata saved to '{csv_path}'")
    print("You can download the subtitled videos and metadata from the file browser.")

def generate_video_metadata(video_name, transcript):
    """
    Generate metadata for a video based on its transcript.
    Returns dictionary with title, description, captions, tags, age restriction.
    """
    # Clean up the video name (remove underscores, dashes, etc.)
    clean_name = re.sub(r'[_-]', ' ', video_name).title()

    # Truncate transcript for caption (max 280 chars)
    caption = transcript[:277] + "..." if len(transcript) > 280 else transcript

    # Generate content-appropriate tags based on transcript
    tags = generate_tags_from_transcript(transcript)

    # Determine age restriction based on content analysis
    age_restriction = "General" if is_content_appropriate(transcript) else "18+"

    # Create a basic description combining video name and transcript
    description = f"{clean_name}\n\n{transcript[:500]}..."
    if len(transcript) > 500:
        description += "\n\n[Full transcript available in captions]"

    # Generate a catchy title
    title = generate_title(clean_name, transcript)

    return {
        "title": title,
        "description": description,
        "captions": caption,
        "tags": tags,
        "age_restriction": age_restriction
    }

def generate_title(video_name, transcript):
    """Generate an engaging title based on video name and transcript content"""
    # Use the video name as base but make it more engaging
    words = transcript.split()

    # Extract potential keywords (simple approach)
    keywords = []
    important_words = ["how", "why", "what", "when", "top", "best", "ultimate", "guide",
                      "tutorial", "review", "tips", "tricks", "secrets", "revealed"]

    # Look for important words in the first 50 words of transcript
    for word in words[:50]:
        clean_word = re.sub(r'[^\w\s]', '', word.lower())
        if len(clean_word) > 4 or clean_word in important_words:
            keywords.append(clean_word)

    keywords = list(set(keywords))[:3]  # Limit to 3 unique keywords

    # Make a title with the video name and some keywords
    if keywords:
        return f"{video_name.title()}: {' '.join(word.title() for word in keywords)}"
    else:
        return f"{video_name.title()} - Full Video"

def generate_tags_from_transcript(transcript):
    """Generate relevant tags based on transcript content"""
    # List of common categories/topics
    categories = {
        "education": ["learn", "study", "education", "school", "college", "university", "tutorial", "guide"],
        "entertainment": ["fun", "funny", "comedy", "entertainment", "laugh", "amusing", "hilarious"],
        "gaming": ["game", "gaming", "play", "player", "level", "minecraft", "fortnite", "gameplay"],
        "music": ["music", "song", "sing", "concert", "band", "artist", "album", "lyrics"],
        "sports": ["sport", "football", "basketball", "soccer", "game", "team", "score", "player"],
        "technology": ["tech", "technology", "computer", "software", "hardware", "programming", "code"],
        "travel": ["travel", "trip", "journey", "vacation", "destination", "tourist", "explore"],
        "cooking": ["food", "recipe", "cook", "cooking", "ingredient", "delicious", "meal", "kitchen"]
    }

    # Basic tags from video type
    tags = ["subtitled", "captions", "highlighted text"]

    # Check transcript for category keywords
    transcript_lower = transcript.lower()
    for category, keywords in categories.items():
        for keyword in keywords:
            if keyword in transcript_lower:
                tags.append(category)
                # Add some keywords as tags too
                tags.extend(keywords[:2])
                break

    # Add some generic popular tags
    tags.extend(["viral", "trending", "2025", "best"])

    # Remove duplicates and limit to 15 tags
    unique_tags = list(set(tags))
    return unique_tags[:15]

def is_content_appropriate(transcript):
    """Check if content is appropriate for general audiences"""
    # List of potentially sensitive words/topics
    sensitive_words = [
        "explicit", "sex", "nude", "nsfw", "alcohol", "drug", "marijuana", "cocaine",
        "violence", "kill", "weapon", "gun", "death", "profanity", "fuck", "shit"
    ]

    # Check for sensitive content
    transcript_lower = transcript.lower()
    for word in sensitive_words:
        if word in transcript_lower:
            return False

    return True

def ensure_roboto_font(work_dir):
    """Download and install Roboto font if needed"""
    import requests
    from pathlib import Path

    # Define font directory and path
    font_dir = os.path.join(work_dir, "fonts")
    os.makedirs(font_dir, exist_ok=True)
    font_path = os.path.join(font_dir, "Roboto-Regular.ttf")

    # Download if not exists
    if not os.path.exists(font_path):
        print("  Downloading Roboto font...")
        font_url = "https://github.com/google/fonts/raw/main/apache/roboto/static/Roboto-Regular.ttf"
        response = requests.get(font_url)

        if response.status_code == 200:
            with open(font_path, 'wb') as f:
                f.write(response.content)
            print("  Roboto font downloaded successfully")
        else:
            print(f"  Failed to download Roboto font: {response.status_code}")
            # Fall back to Arial if download fails
            font_path = "Arial"

    return font_path

def extract_audio(input_video, output_audio):
    """Extract audio from video file at 16kHz mono for Whisper processing"""
    cmd = [
        'ffmpeg', '-y', '-i', input_video,
        '-vn', '-acodec', 'pcm_s16le',
        '-ar', '16000', '-ac', '1',
        '-hide_banner', '-loglevel', 'error',
        output_audio
    ]
    subprocess.run(cmd, check=True)

def format_timestamp(seconds):
    """Convert seconds to ASS timestamp format (h:mm:ss.cc)"""
    td = timedelta(seconds=seconds)
    hours, remainder = divmod(td.seconds, 3600)
    minutes, seconds = divmod(remainder, 60)
    centiseconds = int(td.microseconds / 10000)
    return f"{hours}:{minutes:02d}:{seconds:02d}.{centiseconds:02d}"

def transcribe_with_word_timestamps(model, audio_path):
    """Get word-level timestamps using Whisper"""
    result = model.transcribe(
        audio_path,
        word_timestamps=True,
        verbose=False
    )
    return result

def create_word_highlighting_ass(result, ass_path, font_name):
    """
    Create ASS subtitle file with individual word highlighting
    - Shows one line at a time in the middle of the screen
    - Current word is highlighted in yellow
    - Other words are white
    - Uses specified font (Roboto)
    """
    with open(ass_path, 'w', encoding='utf-8') as f:
        # Write header with styles
        f.write(
            "[Script Info]\n"
            "Title: Word Highlighting Subtitles\n"
            "ScriptType: v4.00+\n"
            "WrapStyle: 0\n"  # Force no line wrapping
            "ScaledBorderAndShadow: yes\n"
            "PlayResX: 1080\n"  # Vertical video typical dimensions
            "PlayResY: 1920\n\n"
            "[V4+ Styles]\n"
            "Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding\n"
            # Default style - white text, centered in middle (Alignment=5 for middle-center)
            f"Style: Default,{font_name},55,&H00FFFFFF,&H000000FF,&H00000000,&H80000000,-1,0,0,0,100,100,0,0,1,2.5,1.5,5,10,10,10,1\n"
            # Highlighted style - yellow text
            f"Style: Highlight,{font_name},55,&H0000FFFF,&H000000FF,&H00000000,&H80000000,-1,0,0,0,100,100,0,0,1,2.5,1.5,5,10,10,10,1\n\n"
            "[Events]\n"
            "Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text\n"
        )

        # Split segments into shorter phrases for single-line display
        processed_segments = []

        for segment in result["segments"]:
            if "words" not in segment or not segment["words"]:
                continue

            words = segment["words"]

            # Group words into smaller phrases (5-6 words max per phrase)
            current_phrase = []
            current_start = None

            for word_data in words:
                word = word_data["word"].strip()
                if not word:
                    continue

                if current_start is None:
                    current_start = word_data["start"]

                current_phrase.append(word_data)

                # After collecting max words or at the end, create a phrase
                if len(current_phrase) >= 6:
                    phrase_end = current_phrase[-1]["end"]
                    processed_segments.append({
                        "start": current_start,
                        "end": phrase_end,
                        "words": current_phrase
                    })
                    current_phrase = []
                    current_start = None

            # Add any remaining words as a final phrase
            if current_phrase:
                phrase_end = current_phrase[-1]["end"]
                processed_segments.append({
                    "start": current_start,
                    "end": phrase_end,
                    "words": current_phrase
                })

        # Process each phrase as a single subtitle line
        for phrase in processed_segments:
            phrase_words = phrase["words"]
            full_text = ' '.join([w["word"].strip() for w in phrase_words if w["word"].strip()])

            # Generate the base subtitle line that displays the full text in white
            phrase_start = format_timestamp(phrase["start"])
            phrase_end = format_timestamp(phrase["end"])

            # Write the base white text line with forced single-line
            clean_text = full_text.replace('\\N', ' ').replace('\n', ' ')
            f.write(f"Dialogue: 0,{phrase_start},{phrase_end},Default,,0,0,0,,{clean_text}\n")

            # Now add individual animations for each word
            for word_data in phrase_words:
                word = word_data["word"].strip()
                if not word:
                    continue

                word_start = word_data["start"]
                word_end = word_data["end"]

                # Find position of this word in the clean text
                word_position = clean_text.find(word)
                if word_position == -1:
                    continue

                # Create timestamps
                word_start_stamp = format_timestamp(word_start)
                word_end_stamp = format_timestamp(word_end)

                # Create an override that highlights just this word
                before_text = clean_text[:word_position]
                after_text = clean_text[word_position + len(word):]

                # Write the highlight effect - this overrides just during the word timing
                highlighted_text = f"{before_text}{{\\c&H00FFFF&}}{word}{{\\c&HFFFFFF&}}{after_text}"
                f.write(f"Dialogue: 1,{word_start_stamp},{word_end_stamp},Default,,0,0,0,,{highlighted_text}\n")

def burn_subtitles(input_video, subs_file, output_video):
    """Burn subtitles into video using ffmpeg"""
    cmd = [
        'ffmpeg', '-y', '-i', input_video,
        '-vf', f"ass={subs_file}",
        '-c:a', 'copy',
        '-c:v', 'libx264',
        '-preset', 'fast',
        '-crf', '23',
        '-hide_banner', '-loglevel', 'error',
        output_video
    ]
    subprocess.run(cmd, check=True)

def download_metadata():
    """Download the metadata CSV file"""
    csv_path = "video_metadata.csv"

    if not os.path.exists(csv_path):
        print(f"Metadata file '{csv_path}' not found. Process videos first.")
        return

    print("Downloading video metadata CSV...")
    files.download(csv_path)

# INSTRUCTIONS:

# 1. Run this cell to process videos with subtitles and generate metadata
add_subtitles_and_generate_metadata(font="Roboto")  # Use Roboto font for subtitles

# 2. Run this cell to download the metadata CSV
download_metadata()

import os
import subprocess
from pathlib import Path
import re
from glob import glob

# Function to convert timestamp string to seconds
def timestamp_to_seconds(timestamp):
    # Handle different timestamp formats
    if ':' in timestamp:
        parts = timestamp.split(':')
        if len(parts) == 2:  # MM:SS format
            return int(parts[0]) * 60 + float(parts[1])
        elif len(parts) == 3:  # HH:MM:SS format
            return int(parts[0]) * 3600 + int(parts[1]) * 60 + float(parts[2])
    return float(timestamp)  # Just seconds

# Function to get video duration using ffprobe
def get_video_duration(video_path):
    cmd = [
        'ffprobe',
        '-v', 'error',
        '-show_entries', 'format=duration',
        '-of', 'default=noprint_wrappers=1:nokey=1',
        video_path
    ]
    result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    return float(result.stdout.strip())

# Function to speed up video
def speed_up_video(input_path, output_path, speed_factor):
    # Make sure speed_factor is reasonable (avoid extreme speeding)
    if speed_factor > 5:
        print(f"Warning: Very high speed factor ({speed_factor:.2f}x) for {input_path}. Limiting to 5x.")
        speed_factor = 5

    # Command to speed up video with audio
    cmd = [
        'ffmpeg',
        '-i', input_path,
        '-filter_complex', f'[0:v]setpts={1/speed_factor}*PTS[v];[0:a]atempo={min(2.0, speed_factor)}' +
                          (f';[0:a]atempo={min(2.0, speed_factor/2.0)}' if speed_factor > 2.0 else '') +
                          (f';[0:a]atempo={speed_factor/4.0}' if speed_factor > 4.0 else '') + '[a]',
        '-map', '[v]',
        '-map', '[a]',
        '-y',  # Overwrite output files without asking
        output_path
    ]

    print(f"Processing: {os.path.basename(input_path)} -> {os.path.basename(output_path)}")
    print(f"Speed factor: {speed_factor:.2f}x (making it 59 seconds)")

    subprocess.run(cmd)
    return output_path

# Set up directories
source_dir = 'subtitled'
output_dir = 'final'

# Create output directory if it doesn't exist
os.makedirs(output_dir, exist_ok=True)

# Find all video files in the source directory
video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.webm']
all_videos = []

for ext in video_extensions:
    all_videos.extend(glob(f"{source_dir}/*{ext}"))

if not all_videos:
    print(f"No videos found in {source_dir} directory!")
else:
    print(f"Found {len(all_videos)} videos to process.")

    # Process each video
    for video_path in all_videos:
        # Get filename without extension
        base_name = os.path.basename(video_path)
        name_without_ext = os.path.splitext(base_name)[0]
        ext = os.path.splitext(base_name)[1]

        # Output path
        output_path = os.path.join(output_dir, f"{name_without_ext}_59sec{ext}")

        # Get current duration
        duration = get_video_duration(video_path)

        # Calculate speed factor needed to make it 59 seconds
        target_duration = 59.0
        speed_factor = duration / target_duration

        # Skip videos that are already 59 seconds or shorter
        if duration <= target_duration:
            print(f"Skipping {base_name}: Already shorter than 59 seconds ({duration:.2f}s)")
            continue

        # Speed up the video
        sped_up_video = speed_up_video(video_path, output_path, speed_factor)

        # Verify new duration
        new_duration = get_video_duration(output_path)
        print(f"New duration: {new_duration:.2f}s (Target: 59.00s)")
        print("-" * 50)

print("Processing complete! All videos have been sped up to 59 seconds and saved to the fast_videos_59sec folder.")



